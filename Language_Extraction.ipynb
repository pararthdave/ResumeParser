{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language-Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pararthdave/ResumeParser/blob/maulik/Language_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pdfminer.six\n",
        "!pip3 install phonenumbers"
      ],
      "metadata": {
        "id": "RhMVCCngT-3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/ResumeParser/Dataset/dataset.zip\"\n",
        "!cp -rv $path ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXi3kRF4vidz",
        "outputId": "6d45a445-ed2c-46c6-ddac-c051b33f8730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/ResumeParser/Dataset/dataset.zip' -> './dataset.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d90LuWppvyrp",
        "outputId": "7b81a86a-7866-433e-e923-88e5ff0518ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/Harika Sonnathi Resume 2022 2.pdf  \n",
            "  inflating: dataset/KetanPawarResume.pdf  \n",
            "  inflating: dataset/Mandar bhoir Resume (1)_compressed.pdf  \n",
            "  inflating: dataset/pararthdave_Resume.pdf  \n",
            "  inflating: dataset/Priyanka_RaikwarRESUME.pdf  \n",
            "  inflating: dataset/priyanka_resume(8).pdf  \n",
            "  inflating: dataset/ravisatvik.192_Resume.pdf  \n",
            "  inflating: dataset/YOGAPRIYA H_RESUME.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "import sys,os,io\n",
        "def split(delimiters, string, maxsplit=0):\n",
        "    import re\n",
        "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
        "    return re.split(regexPattern, string, maxsplit)\n",
        "\n",
        "\n",
        "\n",
        "def get_lang_list():\n",
        "    filePath = './Languages.csv'\n",
        "    df = pd.read_csv(filePath)\n",
        "    # str.replace('; ', ', ') and then a str.split(', ')\n",
        "    df['Language name '].replace(to_replace =\"marathi (marāṭhī)\",\n",
        "                 value =\"marathi\")\n",
        "    # languages = df.iloc[:, [3]].to_list()\n",
        "    lang_list =df['Language name '].to_list()\n",
        "    all_lang_list=[]\n",
        "    for i in lang_list:\n",
        "        i=i.strip().lower()\n",
        "        i = split(' ,;()', i)\n",
        "        if(type(i) == list):\n",
        "            for x in i:\n",
        "                if(x!=''):\n",
        "                    all_lang_list.append(x)\n",
        "        else:\n",
        "            all_lang_list.append(i)\n",
        "    \n",
        "    return all_lang_list\n",
        "\n",
        "def ProperNounExtractor(text):\n",
        "    lst1=[]\n",
        "    string=\" \".join(map(str, text))\n",
        "    string=string.replace('[','')\n",
        "    string=string.replace(']','')\n",
        "    string=string.replace(\"'\",'')\n",
        "    sentences = nltk.sent_tokenize(string)\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tagged = nltk.pos_tag(words)\n",
        "        for (word, tag) in tagged:\n",
        "            if tag == 'NN': # If the word is a proper noun\n",
        "                print(word)\n",
        "lst=get_lang_list()\n",
        "tokenized_sents = [word_tokenize(i) for i in lst]\n",
        "old_stdout = sys.stdout\n",
        "new_stdout = io.StringIO()\n",
        "sys.stdout = new_stdout\n",
        "ProperNounExtractor(tokenized_sents)\n",
        "\n",
        "output = new_stdout.getvalue()\n",
        "\n",
        "sys.stdout = old_stdout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi8OO1e7Swe5",
        "outputId": "cba3d301-3e6b-4183-df3e-4671fb8b0323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.replace('\\n',' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "9oJJPCDJHDaj",
        "outputId": "31268e37-cad6-4f15-bce6-69d3f7e22259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aymara azerbaijani bashkir bihari bislama bambara bengali bangla standard breton bosnian catalan chechen chamorro cree czech church church chuvash welsh divehi dhivehi dzongkha ewe esperanto basque farsi fula fulah pulaar guaraní gujarati manx hausa hindi hiri motu creole herero interlingua interlingue igbo inupiaq ido inuktitut kongo kikuyu gikuyu kwanyama kuanyama kazakh kalaallisut khmer komi kyrgyz letzeburgesch ganda lao malagasy māori malayalam marathi marāṭhī malay nauru bokmål nepali dutch nynorsk south navaho chichewa chewa ojibwe ojibwa oromo panjabi punjabi pāli pashto pushto quechua romansh kirundi kinyarwanda sanskrit saṁskṛta sindhi sami sango slovene samoan shona somali swati sotho swahili tamil telugu tajik thai tonga tonga tsonga tatar twi uyghur urdu venda volapük walloon wolof zulu '"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j7_xs3QvH49W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrreNTd3T9tl",
        "outputId": "7edd5984-3c07-4e04-f351-8ae0dccaea18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[]\n",
            "[]\n",
            "['Standard']\n",
            "[]\n",
            "[]\n",
            "['Telugu', 'Central']\n",
            "['Japanese', 'Hindi', 'Marathi', 'English']\n",
            "['Modern']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import subprocess\n",
        "import os\n",
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text\n",
        "import phonenumbers\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import pprint\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "RESERVED_WORDS = [\n",
        "    'achievement',\n",
        "    'accomplishment',\n",
        "]\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def doc_to_text_catdoc(file_path):\n",
        "    try:\n",
        "        process = subprocess.Popen(  # noqa: S607,S603\n",
        "            ['catdoc', '-w', file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "    except (\n",
        "        FileNotFoundError,\n",
        "        ValueError,\n",
        "        subprocess.TimeoutExpired,\n",
        "        subprocess.SubprocessError,\n",
        "    ) as err:\n",
        "        return (None, str(err))\n",
        "    else:\n",
        "        stdout, stderr = process.communicate()\n",
        "\n",
        "    return (stdout.strip(), stderr.strip())\n",
        "\n",
        "def matchLang(lang):\n",
        "    languages = lang\n",
        "\n",
        "    pattern = [{'POS': 'PROPN'}]\n",
        "\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add('langMatch', [pattern])\n",
        "    \n",
        "    for lang in languages:\n",
        "        matches.matcher(nlp(lang))\n",
        "    matches = matcher(nlp(languages))\n",
        "    \n",
        "    # print(matches)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        print(span.text)\n",
        "        return span.text\n",
        "\n",
        "def extract_language(input_text):\n",
        "    languages = get_lang_list()\n",
        "    # print(languages)\n",
        "    nlp_text = nlp(input_text)\n",
        "    noun_chunks = nlp_text.noun_chunks\n",
        "    # print(type(noun_chunks))\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    master_language = []\n",
        "\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.lower() in languages:\n",
        "            master_language.append(token)\n",
        "    \n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        # print(token)\n",
        "        for lang in languages:\n",
        "            if(token==lang):\n",
        "                master_language.append(token)\n",
        "            # if(lang.find(token) != -1):\n",
        "            #     master_language.append(token)\n",
        "\n",
        "        # if token in languages:\n",
        "        #     master_language.append(token)\n",
        "    return [i.capitalize() for i in set([i.lower() for i in master_language])]\n",
        "\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    directory = '/content/dataset/'\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        f = os.path.join(directory, filename)\n",
        "        text = extract_text_from_pdf(f)\n",
        "        # print(matchLang())\n",
        "        print(extract_language(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhi4I64uvUDl",
        "outputId": "4952066a-50b3-4ff1-a157-9cd10aab8ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'996 668'.split()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aeiOthyCiqoG",
        "outputId": "7efc47d6-3c13-45e5-dd16-01dc4df18e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'996'"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}