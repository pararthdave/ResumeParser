{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeParser_ExtractExperience.ipynb",
      "provenance": [],
      "mount_file_id": "15G3_OP5vquUxFthRUytQhENifaZecUza",
      "authorship_tag": "ABX9TyMoBS47RhRrMppNj1Qic+jV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pararthdave/ResumeParser/blob/RaviGorthi/ResumeParser_ExtractExperience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "import re\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.rrule import rrule, MONTHLY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGLtRVK0l6L9",
        "outputId": "aa5a1c9a-e5fe-47e6-9ae4-a2b0b80e6fc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IuIVxuuIlFqf"
      },
      "outputs": [],
      "source": [
        "def extract_experience(pdf_text):\n",
        "  # Create sentance tokens from the text\n",
        "  lines = [el.strip() for el in pdf_text.split(\"\\n\") if len(el) > 0]\n",
        "  # Create word tokens from sentances\n",
        "  lines = [nltk.word_tokenize(el) for el in lines]\n",
        "  # Assign \"Part of Speech\" tags to each word\n",
        "  lines = [nltk.pos_tag(el) for el in lines]\n",
        "\n",
        "  # Initialise date variables with a random date\n",
        "  experience = '1/1/2000'\n",
        "  education = '1/1/2000'\n",
        "\n",
        "  # Capture sentances which contain the term \"Experience\" and \"Education\"\n",
        "\n",
        "  for sentance in lines:\n",
        "    # Discard tags from words and join all words\n",
        "    sen = \" \".join([words[0].lower() for words in sentance])\n",
        "\n",
        "\n",
        "    if re.search('experience',sen):\n",
        "      sen_tokenised = nltk.word_tokenize(sen)\n",
        "      tagged = nltk.pos_tag(sen_tokenised)\n",
        "      # Create chunks of data using the recommended named entity chunker\n",
        "      entities = nltk.chunk.ne_chunk(tagged)\n",
        "      for subtree in entities.subtrees():\n",
        "        for leaf in subtree.leaves():\n",
        "          if leaf[1] == 'CD':\n",
        "            # Extract Date from that sentance\n",
        "            experience = leaf[0]\n",
        "\n",
        "    if re.search('education',sen):\n",
        "      sen_tokenised = nltk.word_tokenize(sen)\n",
        "      tagged = nltk.pos_tag(sen_tokenised)\n",
        "      entities = nltk.chunk.ne_chunk(tagged)\n",
        "      for subtree in entities.subtrees():\n",
        "        for leaf in subtree.leaves():\n",
        "          if leaf[1] == 'CD':\n",
        "            # Extract Date from that sentance\n",
        "            education = leaf[0]\n",
        "\n",
        "  # Convert the dates to DateTime objects\n",
        "  date_time_obj_edu = datetime.strptime(education, '%d/%m/%Y')\n",
        "  date_time_obj_exp = datetime.strptime(experience, '%d/%m/%Y')\n",
        "\n",
        "  # Get number of months between the two dates\n",
        "  strt_dt = date_time_obj_edu\n",
        "  end_dt = date_time_obj_exp\n",
        "  dates = [dt for dt in rrule(MONTHLY, dtstart=strt_dt, until=end_dt)]\n",
        "\n",
        "  # Return experience as number of months\n",
        "  print (\"Experience in Months: \")\n",
        "  return (len(dates) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of “noun phrases.” It is used to add structure to the sentence by following POS tagging combined with regular expressions. The resulted group of words are called “chunks.” It is also called shallow parsing.\n",
        "\n",
        "In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow parsing is also called light parsing or chunking.\n",
        "\n"
      ],
      "metadata": {
        "id": "D2ramgVGoWgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Driver code for Testing"
      ],
      "metadata": {
        "id": "yLm4ZE0hvpMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvgWVnDqwWhk",
        "outputId": "3ab7ce3d-42c5-42dc-e7cc-e0f4126d0d1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.1 pdfminer.six-20211012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "directory = '/content/drive/MyDrive/MyDocuments/ResumeParser/Team_Resumes'\n",
        "filename = 'priyanka_resume(8).pdf'\n",
        "f = os.path.join(directory, filename)\n",
        "text = extract_text(f)"
      ],
      "metadata": {
        "id": "-eD9Q3ASvmnZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_experience(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMGogrulwZvR",
        "outputId": "f4604bcc-4e14-496c-f8b2-d8989f6e0e43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experience in Months: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3OxQbswTDjFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}