{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeParser_ExtractExperience.ipynb",
      "provenance": [],
      "mount_file_id": "15G3_OP5vquUxFthRUytQhENifaZecUza",
      "authorship_tag": "ABX9TyPazi4lTG65opVpnhMjGyDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pararthdave/ResumeParser/blob/RaviGorthi/ResumeParser_ExtractExperience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "import re\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.rrule import rrule, MONTHLY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGLtRVK0l6L9",
        "outputId": "2a5f72b0-8e44-43e3-82d3-82e8031a1327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuIVxuuIlFqf"
      },
      "outputs": [],
      "source": [
        "def extract_experience(pdf_text):\n",
        "  # Create sentance tokens from the text\n",
        "  lines = [el.strip() for el in pdf_text.split(\"\\n\") if len(el) > 0]\n",
        "  # Create word tokens from sentances\n",
        "  lines = [nltk.word_tokenize(el) for el in lines]\n",
        "  # Assign \"Part of Speech\" tags to each word\n",
        "  lines = [nltk.pos_tag(el) for el in lines]\n",
        "\n",
        "\n",
        "  # Capture sentances which contain the term \"Experience\" and \"Education\"\n",
        "\n",
        "  for sentance in lines:\n",
        "    # Discard tags from words and join all words\n",
        "    sen = \" \".join([words[0].lower() for words in sentance])\n",
        "\n",
        "\n",
        "    if re.search('experience',sen):\n",
        "      sen_tokenised = nltk.word_tokenize(sen)\n",
        "      tagged = nltk.pos_tag(sen_tokenised)\n",
        "      # Create chunks of data using the recommended named entity chunker\n",
        "      entities = nltk.chunk.ne_chunk(tagged)\n",
        "      for subtree in entities.subtrees():\n",
        "        for leaf in subtree.leaves():\n",
        "          if leaf[1] == 'CD':\n",
        "            # Extract Date from that sentance\n",
        "            experience = leaf[0]\n",
        "\n",
        "    if re.search('education',sen):\n",
        "      sen_tokenised = nltk.word_tokenize(sen)\n",
        "      tagged = nltk.pos_tag(sen_tokenised)\n",
        "      entities = nltk.chunk.ne_chunk(tagged)\n",
        "      for subtree in entities.subtrees():\n",
        "        for leaf in subtree.leaves():\n",
        "          if leaf[1] == 'CD':\n",
        "            # Extract Date from that sentance\n",
        "            education = leaf[0]\n",
        "\n",
        "  # Convert the dates to DateTime objects\n",
        "  date_time_obj_edu = datetime.strptime(education, '%d/%m/%Y')\n",
        "  date_time_obj_exp = datetime.strptime(experience, '%d/%m/%Y')\n",
        "\n",
        "  # Get number of months between the two dates\n",
        "  strt_dt = date_time_obj_edu\n",
        "  end_dt = date_time_obj_exp\n",
        "  dates = [dt for dt in rrule(MONTHLY, dtstart=strt_dt, until=end_dt)]\n",
        "\n",
        "  # Return experience as number of months\n",
        "  print (\"Experience in Months: \")\n",
        "  return (len(dates) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of “noun phrases.” It is used to add structure to the sentence by following POS tagging combined with regular expressions. The resulted group of words are called “chunks.” It is also called shallow parsing.\n",
        "\n",
        "In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow parsing is also called light parsing or chunking.\n",
        "\n"
      ],
      "metadata": {
        "id": "D2ramgVGoWgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Driver code for Testing"
      ],
      "metadata": {
        "id": "yLm4ZE0hvpMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvgWVnDqwWhk",
        "outputId": "20f1215a-6a30-4316-ac39-7bff1c8dbb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "directory = '/content/drive/MyDrive/MyDocuments/ResumeParser/Team_Resumes'\n",
        "filename = 'pararthdave_Resume.pdf'\n",
        "f = os.path.join(directory, filename)\n",
        "text = extract_text(f)"
      ],
      "metadata": {
        "id": "-eD9Q3ASvmnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_experience(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMGogrulwZvR",
        "outputId": "d928fe04-b249-429c-bf29-0e949434bee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experience in Months: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}