{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pararthdave/ResumeParser/blob/maulik/language_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pdfminer.six\n",
        "!pip install gdown\n",
        "!pip install glob"
      ],
      "metadata": {
        "id": "RhMVCCngT-3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c942951-e8bf-40cc-cccf-1d72b8ffef2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for glob\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q9eiLLJyq9np"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download_folder(\"https://drive.google.com/drive/folders/1HI8qzc6mhWtKe83B3600uj9ELVny3Pih\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXi3kRF4vidz",
        "outputId": "bbb8d1f2-971e-4a55-f58a-b3bde677dbde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1f3V8vAOhqaNzUrjAnKG8-BQ0j37yryoz 201801247_Alay_Kharadi.pdf\n",
            "Processing file 1jb4pkhdR1ou52G67NtoQFKocjI_VX4Ac 202012063_Kushal_Tanna.pdf\n",
            "Processing file 14DQhZ1zTJ9u3vKHMTAmP4OCCkC-ttoa3 Harika Sonnathi Resume 2022 2.pdf\n",
            "Processing file 1JoWitxf9uPb0QpB8JAPKyqBk7FcgQ-9U KetanPawarResume.pdf\n",
            "Processing file 165kSVmORNL08pmp2Vt_c4DDPxf8lBYu5 Languages.csv\n",
            "Processing file 1LRbrdnHJZz4i6JRkCw53FI54wDkKLn2a Mandar bhoir Resume (1)_compressed.pdf\n",
            "Processing file 1WpZYKa50BVikBvEUSYt0uHXt0qn2cxNf pararthdave_Resume.pdf\n",
            "Processing file 1p3ExSxqOsA60MmGDx3aVh_ZsxXZxbWZf Priyanka_RaikwarRESUME.pdf\n",
            "Processing file 10UGCWDkQBtNpYnMXCMvf0znwxCOyfB9V priyanka_resume(8).pdf\n",
            "Processing file 1SGaYDOFK2rhW_hQNj55NO4Q413590FyD ravisatvik.192_Resume.pdf\n",
            "Processing file 1H2vs3vVu0AV_km41lVGxsm2Qqq-Fhjqj YOGAPRIYA H_RESUME.pdf\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3V8vAOhqaNzUrjAnKG8-BQ0j37yryoz\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/201801247_Alay_Kharadi.pdf\n",
            "100%|██████████| 45.8k/45.8k [00:00<00:00, 3.59MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jb4pkhdR1ou52G67NtoQFKocjI_VX4Ac\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/202012063_Kushal_Tanna.pdf\n",
            "100%|██████████| 520k/520k [00:00<00:00, 5.52MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14DQhZ1zTJ9u3vKHMTAmP4OCCkC-ttoa3\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/Harika Sonnathi Resume 2022 2.pdf\n",
            "100%|██████████| 455k/455k [00:00<00:00, 5.05MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JoWitxf9uPb0QpB8JAPKyqBk7FcgQ-9U\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/KetanPawarResume.pdf\n",
            "100%|██████████| 278k/278k [00:00<00:00, 3.02MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=165kSVmORNL08pmp2Vt_c4DDPxf8lBYu5\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/Languages.csv\n",
            "100%|██████████| 8.29k/8.29k [00:00<00:00, 11.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LRbrdnHJZz4i6JRkCw53FI54wDkKLn2a\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/Mandar bhoir Resume (1)_compressed.pdf\n",
            "100%|██████████| 114k/114k [00:00<00:00, 2.29MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WpZYKa50BVikBvEUSYt0uHXt0qn2cxNf\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/pararthdave_Resume.pdf\n",
            "100%|██████████| 610k/610k [00:00<00:00, 5.09MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1p3ExSxqOsA60MmGDx3aVh_ZsxXZxbWZf\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/Priyanka_RaikwarRESUME.pdf\n",
            "100%|██████████| 385k/385k [00:00<00:00, 3.67MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10UGCWDkQBtNpYnMXCMvf0znwxCOyfB9V\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/priyanka_resume(8).pdf\n",
            "100%|██████████| 49.3k/49.3k [00:00<00:00, 3.24MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SGaYDOFK2rhW_hQNj55NO4Q413590FyD\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/ravisatvik.192_Resume.pdf\n",
            "100%|██████████| 95.9k/95.9k [00:00<00:00, 2.12MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1H2vs3vVu0AV_km41lVGxsm2Qqq-Fhjqj\n",
            "To: /content/ResumeParserDataset/ResumeParserDataset/YOGAPRIYA H_RESUME.pdf\n",
            "100%|██████████| 202k/202k [00:00<00:00, 2.42MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/ResumeParserDataset/ResumeParserDataset/201801247_Alay_Kharadi.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/202012063_Kushal_Tanna.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/Harika Sonnathi Resume 2022 2.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/KetanPawarResume.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/Languages.csv',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/Mandar bhoir Resume (1)_compressed.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/pararthdave_Resume.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/Priyanka_RaikwarRESUME.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/priyanka_resume(8).pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/ravisatvik.192_Resume.pdf',\n",
              " '/content/ResumeParserDataset/ResumeParserDataset/YOGAPRIYA H_RESUME.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import sys,os,io\n",
        "def split(delimiters, string, maxsplit=0):\n",
        "    import re\n",
        "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
        "    return re.split(regexPattern, string, maxsplit)\n",
        "\n",
        "\n",
        "\n",
        "def get_lang_list():\n",
        "    filePath = '/content/Languages.csv'\n",
        "    df = pd.read_csv(filePath)\n",
        "    # str.replace('; ', ', ') and then a str.split(', ')\n",
        "    df['Language name '].replace(to_replace =\"marathi (marāṭhī)\",\n",
        "                 value =\"marathi\")\n",
        "    # languages = df.iloc[:, [3]].to_list()\n",
        "    lang_list =df['Language name '].to_list()\n",
        "    all_lang_list=[]\n",
        "    for i in lang_list:\n",
        "        i=i.strip().lower()\n",
        "        i = split(' ,;()', i)\n",
        "        if(type(i) == list):\n",
        "            for x in i:\n",
        "                if(x!=''):\n",
        "                    all_lang_list.append(x)\n",
        "        else:\n",
        "            all_lang_list.append(i)\n",
        "    wordsToRemove = [\"standard\", \"central\", \"modern\"]\n",
        "    for word in wordsToRemove:\n",
        "        all_lang_list.remove(word)\n",
        "    \n",
        "    return all_lang_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi8OO1e7Swe5",
        "outputId": "0bb07fbf-9390-425b-cd15-17960a0a6037"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_lang_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIaTBdH3lRKT",
        "outputId": "0c4f38e5-a2b5-4a03-d424-6546e1c38456"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['afar',\n",
              " 'abkhaz',\n",
              " 'avestan',\n",
              " 'afrikaans',\n",
              " 'akan',\n",
              " 'amharic',\n",
              " 'aragonese',\n",
              " 'arabic',\n",
              " 'assamese',\n",
              " 'avaric',\n",
              " 'aymara',\n",
              " 'azerbaijani',\n",
              " 'south',\n",
              " 'azerbaijani',\n",
              " 'bashkir',\n",
              " 'belarusian',\n",
              " 'bulgarian',\n",
              " 'bihari',\n",
              " 'bislama',\n",
              " 'bambara',\n",
              " 'bengali',\n",
              " 'bangla',\n",
              " 'tibetan',\n",
              " 'tibetan',\n",
              " 'breton',\n",
              " 'bosnian',\n",
              " 'catalan',\n",
              " 'valencian',\n",
              " 'chechen',\n",
              " 'chamorro',\n",
              " 'corsican',\n",
              " 'cree',\n",
              " 'czech',\n",
              " 'old',\n",
              " 'church',\n",
              " 'slavonic',\n",
              " 'church',\n",
              " 'slavonic',\n",
              " 'old',\n",
              " 'bulgarian',\n",
              " 'chuvash',\n",
              " 'welsh',\n",
              " 'danish',\n",
              " 'german',\n",
              " 'divehi',\n",
              " 'dhivehi',\n",
              " 'maldivian',\n",
              " 'dzongkha',\n",
              " 'ewe',\n",
              " 'greek',\n",
              " 'english',\n",
              " 'esperanto',\n",
              " 'spanish',\n",
              " 'castilian',\n",
              " 'estonian',\n",
              " 'basque',\n",
              " 'persian',\n",
              " 'farsi',\n",
              " 'fula',\n",
              " 'fulah',\n",
              " 'pulaar',\n",
              " 'pular',\n",
              " 'finnish',\n",
              " 'fijian',\n",
              " 'faroese',\n",
              " 'french',\n",
              " 'western',\n",
              " 'frisian',\n",
              " 'irish',\n",
              " 'scottish',\n",
              " 'gaelic',\n",
              " 'gaelic',\n",
              " 'galician',\n",
              " 'guaraní',\n",
              " 'gujarati',\n",
              " 'manx',\n",
              " 'hausa',\n",
              " 'hebrew',\n",
              " 'modern',\n",
              " 'hindi',\n",
              " 'hiri',\n",
              " 'motu',\n",
              " 'croatian',\n",
              " 'haitian',\n",
              " 'haitian',\n",
              " 'creole',\n",
              " 'hungarian',\n",
              " 'armenian',\n",
              " 'herero',\n",
              " 'interlingua',\n",
              " 'indonesian',\n",
              " 'interlingue',\n",
              " 'igbo',\n",
              " 'nuosu',\n",
              " 'inupiaq',\n",
              " 'ido',\n",
              " 'icelandic',\n",
              " 'italian',\n",
              " 'inuktitut',\n",
              " 'japanese',\n",
              " 'javanese',\n",
              " 'georgian',\n",
              " 'kongo',\n",
              " 'kikuyu',\n",
              " 'gikuyu',\n",
              " 'kwanyama',\n",
              " 'kuanyama',\n",
              " 'kazakh',\n",
              " 'kalaallisut',\n",
              " 'greenlandic',\n",
              " 'khmer',\n",
              " 'kannada',\n",
              " 'korean',\n",
              " 'kanuri',\n",
              " 'kashmiri',\n",
              " 'kurdish',\n",
              " 'komi',\n",
              " 'cornish',\n",
              " 'kyrgyz',\n",
              " 'latin',\n",
              " 'luxembourgish',\n",
              " 'letzeburgesch',\n",
              " 'ganda',\n",
              " 'limburgish',\n",
              " 'limburgan',\n",
              " 'limburger',\n",
              " 'lingala',\n",
              " 'lao',\n",
              " 'lithuanian',\n",
              " 'luba-katanga',\n",
              " 'latvian',\n",
              " 'malagasy',\n",
              " 'marshallese',\n",
              " 'māori',\n",
              " 'macedonian',\n",
              " 'malayalam',\n",
              " 'mongolian',\n",
              " 'marathi',\n",
              " 'marāṭhī',\n",
              " 'malay',\n",
              " 'maltese',\n",
              " 'burmese',\n",
              " 'nauru',\n",
              " 'norwegian',\n",
              " 'bokmål',\n",
              " 'north',\n",
              " 'ndebele',\n",
              " 'nepali',\n",
              " 'ndonga',\n",
              " 'dutch',\n",
              " 'norwegian',\n",
              " 'nynorsk',\n",
              " 'norwegian',\n",
              " 'south',\n",
              " 'ndebele',\n",
              " 'navajo',\n",
              " 'navaho',\n",
              " 'chichewa',\n",
              " 'chewa',\n",
              " 'nyanja',\n",
              " 'occitan',\n",
              " 'ojibwe',\n",
              " 'ojibwa',\n",
              " 'oromo',\n",
              " 'oriya',\n",
              " 'ossetian',\n",
              " 'ossetic',\n",
              " 'panjabi',\n",
              " 'punjabi',\n",
              " 'pāli',\n",
              " 'polish',\n",
              " 'pashto',\n",
              " 'pushto',\n",
              " 'portuguese',\n",
              " 'quechua',\n",
              " 'romansh',\n",
              " 'kirundi',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'kinyarwanda',\n",
              " 'sanskrit',\n",
              " 'saṁskṛta',\n",
              " 'sardinian',\n",
              " 'sindhi',\n",
              " 'northern',\n",
              " 'sami',\n",
              " 'sango',\n",
              " 'sinhala',\n",
              " 'sinhalese',\n",
              " 'slovak',\n",
              " 'slovene',\n",
              " 'samoan',\n",
              " 'shona',\n",
              " 'somali',\n",
              " 'albanian',\n",
              " 'serbian',\n",
              " 'swati',\n",
              " 'southern',\n",
              " 'sotho',\n",
              " 'sundanese',\n",
              " 'swedish',\n",
              " 'swahili',\n",
              " 'tamil',\n",
              " 'telugu',\n",
              " 'tajik',\n",
              " 'thai',\n",
              " 'tigrinya',\n",
              " 'turkmen',\n",
              " 'tagalog',\n",
              " 'tswana',\n",
              " 'tonga',\n",
              " 'tonga',\n",
              " 'islands',\n",
              " 'turkish',\n",
              " 'tsonga',\n",
              " 'tatar',\n",
              " 'twi',\n",
              " 'tahitian',\n",
              " 'uyghur',\n",
              " 'uighur',\n",
              " 'ukrainian',\n",
              " 'urdu',\n",
              " 'uzbek',\n",
              " 'venda',\n",
              " 'vietnamese',\n",
              " 'volapük',\n",
              " 'walloon',\n",
              " 'wolof',\n",
              " 'xhosa',\n",
              " 'yiddish',\n",
              " 'yoruba',\n",
              " 'zhuang',\n",
              " 'chuang',\n",
              " 'chinese',\n",
              " 'zulu']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrreNTd3T9tl",
        "outputId": "d4c3a5b0-13b2-40fc-8aaa-3ff8bbc9b8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "['202012063_Kushal_Tanna.pdf', 'KetanPawarResume.pdf', 'Priyanka_RaikwarRESUME.pdf', 'pararthdave_Resume.pdf', 'priyanka_resume(8).pdf', 'Harika Sonnathi Resume 2022 2.pdf', '201801247_Alay_Kharadi.pdf', 'Mandar bhoir Resume (1)_compressed.pdf', 'YOGAPRIYA H_RESUME.pdf', 'ravisatvik.192_Resume.pdf']\n",
            "[]\n",
            "['Marathi', 'Japanese', 'Hindi', 'English']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['Telugu']\n",
            "[]\n",
            "['Modern']\n",
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import subprocess\n",
        "import os\n",
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text\n",
        "import phonenumbers\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import pprint\n",
        "from spacy.matcher import Matcher\n",
        "import glob\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "RESERVED_WORDS = [\n",
        "    'achievement',\n",
        "    'accomplishment',\n",
        "]\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def doc_to_text_catdoc(file_path):\n",
        "    try:\n",
        "        process = subprocess.Popen(  # noqa: S607,S603\n",
        "            ['catdoc', '-w', file_path],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "    except (\n",
        "        FileNotFoundError,\n",
        "        ValueError,\n",
        "        subprocess.TimeoutExpired,\n",
        "        subprocess.SubprocessError,\n",
        "    ) as err:\n",
        "        return (None, str(err))\n",
        "    else:\n",
        "        stdout, stderr = process.communicate()\n",
        "\n",
        "    return (stdout.strip(), stderr.strip())\n",
        "\n",
        "\n",
        "def extract_language(input_text):\n",
        "    languages = get_lang_list()\n",
        "    # print(languages)\n",
        "    nlp_text = nlp(input_text)\n",
        "    noun_chunks = nlp_text.noun_chunks\n",
        "    # print(type(noun_chunks))\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    master_language = []\n",
        "\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.lower() in languages:\n",
        "            master_language.append(token)\n",
        "    \n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        # print(token)\n",
        "        for lang in languages:\n",
        "            if(token==lang):\n",
        "                master_language.append(token)\n",
        "            # if(lang.find(token) != -1):\n",
        "            #     master_language.append(token)\n",
        "\n",
        "        # if token in languages:\n",
        "        #     master_language.append(token)\n",
        "    return [i.capitalize() for i in set([i.lower() for i in master_language])]\n",
        "\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    directory = '/content/ResumeParserDataset'\n",
        "    os.chdir(directory)\n",
        "    files = glob.glob(\"*.pdf\")\n",
        "    print(files)\n",
        "    for filename in files:\n",
        "        f = os.path.join(directory, filename)\n",
        "        text = extract_text_from_pdf(f)\n",
        "        print(extract_language(text))"
      ]
    }
  ]
}